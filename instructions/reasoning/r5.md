## Build search functionality ##

**This involves two types of search.**

1.  **Keyword Search:** Find chunks containing specific words or phrases. This is often useful for exact matches or finding specific terms. We'll primarily target the `document_chunks` table in SQLite. We could use simple `LIKE` queries or potentially explore SQLite's Full-Text Search (FTS) extensions for better performance/relevance if needed later. Let's start with `LIKE`.
2.  **Semantic Search (Vector Search):** Find chunks whose meaning is similar to the query text, even if they don't share exact keywords. This leverages the embeddings stored in ChromaDB.

We need to:
1.  Create new API endpoints in `app/api/endpoints/documents.py` (or potentially a new `search.py` router, but keeping it in `documents` might be okay for now as it searches *within* documents). Let's keep it in `documents.py` for simplicity initially.
2.  Define request and response models for the search queries and results.
3.  Implement the logic for keyword search using SQLAlchemy Core against the SQLite `document_chunks` table.
4.  Implement the logic for semantic search:
    *   Use the `embedding_service` to generate an embedding for the user's query text.
    *   Use the ChromaDB client to query the `rag_documents` collection with the generated query embedding.
    *   Process and format the results from ChromaDB.

**Plan:**

1.  **Define Pydantic models:**
    *   `SearchQuery`: Input model containing the query text and maybe parameters like `top_k`.
    *   `ChunkResult`: Represents a single search result chunk (content, metadata, score/distance).
    *   `SearchResponse`: Output model containing a list of `ChunkResult`.
2.  **Add endpoints to `documents.py` router:**
    *   `POST /api/v1/documents/search/keyword`
    *   `POST /api/v1/documents/search/semantic`
    (Using POST as queries might get long, and it aligns well with sending a JSON body).
3.  **Implement Keyword Search Logic:**
    *   Use `database.fetch_all` with a `SELECT` query on `document_chunks_table`.
    *   Use `sqlalchemy.sql.text` or `ilike` for case-insensitive keyword matching on `text_content`.
    *   Potentially join with `documents_table` to get filename etc., although chunk metadata might suffice. Let's stick to `document_chunks` for now and include `document_id`.
4.  **Implement Semantic Search Logic:**
    *   Get the embedding model using `get_embedding_model`.
    *   Generate query embedding using `generate_embeddings` (or directly `model.encode`). Remember the blocking nature - might need `run_in_executor` later for production.
    *   Get the ChromaDB client and collection.
    *   Call `collection.query()` with the query embedding, `n_results`, and `include` options (documents, metadatas, distances).
    *   Map ChromaDB results to our `ChunkResult` model. Include distance/score.
