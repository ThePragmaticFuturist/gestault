## designing the multi-LLM backend integration ##

1.    **Understand the Goal:** The user wants to connect the existing RAG server to *external* LLM inference servers (Ollama, vLLM, potentially InstructLab's server component) in addition to the current Hugging Face local loading mechanism. The server should be able to *switch* between these backends.

2.    **Identify Core Requirements:**
    *   **Configuration:** Need a way to specify *which* backend to use (e.g., "local", "ollama", "vllm") and the necessary connection details (e.g., base URL for API backends).
    *   **Abstraction:** Need a unified interface within the server code so that the chat endpoint (`add_message_to_session`) doesn't need complex `if/elif` statements for each backend. It should just call a generic "generate" function.
    *   **Backend Implementations:** Need specific logic for interacting with each type of backend (local transformers, Ollama API, vLLM OpenAI-compatible API).
    *   **Switching Mechanism:** Need a way to select and activate the desired backend based on the configuration.
    *   **API Interaction:** For API-based backends (Ollama, vLLM), need an HTTP client library (like `httpx`).
    *   **Error Handling:** Consistent error handling across different backends.
    *   **Status Reporting:** The `/models/status` endpoint should reflect which backend is active and its specific details (model name, URL, etc.).

3.  **Design the Abstraction:**
    *   Define a base class or a common function signature for generation. Let's go with a class-based approach for better state management per backend.
    *   Create an abstract base class `LLMBackendBase` with an `async generate(prompt: str, config: Dict[str, Any]) -> Optional[str]` method. Make it `async` because API calls will be async. Pass the generation `config` explicitly.
    *   Subclass this for each backend:
        *   `LocalTransformersBackend(LLMBackendBase)`: Encapsulates the existing `transformers.pipeline` logic. Needs access to the pipeline object.
        *   `OllamaBackend(LLMBackendBase)`: Takes an Ollama base URL and model name. Uses `httpx` to call Ollama's `/api/generate` endpoint. Needs to map parameters.
        *   `VLLMBackend(LLMBackendBase)`: Takes a vLLM base URL and model name. Uses `httpx` to call vLLM's OpenAI-compatible `/v1/completions` or `/v1/chat/completions` endpoint. Needs to map parameters.
        *   `InstructLabBackend(LLMBackendBase)`: (Placeholder for now, assume similar OpenAI-compatible API).

4.  **Refactor `llm_service.py`:**
    *   This service should now manage the *active* backend instance instead of just the local pipeline.
    *   Introduce a configuration setting (`LLM_BACKEND_TYPE`) to choose the backend.
    *   Modify `llm_state` to store the *active backend instance* (`llm_state["backend_instance"]`).
    *   Modify the loading logic:
        *   The `/load` endpoint now needs to know *which type* of backend to load/configure. The `model_name_or_path` might now represent a model *on that backend* (e.g., "llama3" for Ollama) or a path/ID for local.
        *   The `_load_model_task` needs to be replaced or heavily modified. Maybe have separate load functions per backend type? Or a factory function?
        *   Let's simplify: Instead of `/load` dynamically loading different *types*, let's configure the desired backend *type* and its *URL* in `settings`. The `/load` endpoint will then primarily load/configure the *model* for the *currently configured* backend type.
        *   Local loading remains similar (using `run_in_executor`).
        *   API backends don't need "loading" in the same way; they just need configuration (URL, model name). The `/load` endpoint could be used to *select* the model *on* the API backend and maybe verify the connection.
    *   Modify `generate_text`: This function becomes much simpler. It just gets the active `backend_instance` from `llm_state` and calls `await backend_instance.generate(prompt, config)`.
    *   Modify `get_llm_status`: Needs to report the active backend type, URL (if applicable), and the model selected on that backend.
    *   Modify `update_llm_config`: Should update the config stored in `llm_state`, which will be passed to the active backend's `generate` method.

5.  **Update Configuration (`config.py`):**
    *   Add `LLM_BACKEND_TYPE: str = Field(default="local", description="'local', 'ollama', 'vllm'")`
    *   Add `OLLAMA_BASE_URL: Optional[str] = Field(default=None, description="Base URL for Ollama API (e.g., http://localhost:11434)")`
    *   Add `VLLM_BASE_URL: Optional[str] = Field(default=None, description="Base URL for vLLM OpenAI-compatible API (e.g., http://localhost:8000)")`
    *   The existing `DEFAULT_MODEL_NAME` might need context â€“ is it the local path or the model name on the API server? Let the `/load` request specify the model relevant to the active backend.

6.  **Implement API Backends:**
    *   Install `httpx`: `pip install httpx`.
    *   **Ollama:**
        *   Need the `/api/generate` endpoint structure (model, prompt, stream=False, options={...}).
        *   Map our config (temp, top_k, etc.) to Ollama's `options`.
        *   Handle potential errors from the API call.
    *   **vLLM (OpenAI compatible):**
        *   Decide between `/completions` (legacy) or `/chat/completions`. `/chat/completions` is generally preferred now.
        *   Need to format the prompt into the chat message structure (`messages=[{"role": "user", "content": prompt}]`).
        *   Map our config to OpenAI parameters (temperature, top_p, max_tokens). Note `top_k` isn't standard in OpenAI chat API.
        *   Handle API errors.

7.  **Refine `load_model` Endpoint:**
    *   The request body should still contain `model_name_or_path`.
    *   The endpoint logic checks `settings.LLM_BACKEND_TYPE`.
    *   If "local": Calls the existing local loading task (maybe renamed `_load_local_model_task`).
    *   If "ollama" or "vllm":
        *   Checks if the corresponding base URL is configured.
        *   Updates `llm_state` with the chosen model name, backend type, and URL. Sets status to `LOADED` (since no real loading needed, just configuration/selection).
        *   Maybe perform a quick health check ping to the API endpoint? (e.g., list models on Ollama/vLLM).

8.  **Update API Models (`llm.py`):**
    *   `ModelLoadRequest`: Keep `model_name_or_path`. Remove `device` and `quantization` as they are less relevant/handled differently for API backends (or make them optional).
    *   `ModelStatusResponse`: Add `backend_type`, `backend_url`. Rename `model_name_or_path` to `active_model`.

9.  **Refactor `sessions.py`:** No changes should be needed here, as it calls the abstracted `generate_text` from `llm_service`.

10. **Testing:** Test each backend type:
    *   Configure `LLM_BACKEND_TYPE="local"` -> load `gpt2` -> chat.
    *   Configure `LLM_BACKEND_TYPE="ollama"`, `OLLAMA_BASE_URL="http://localhost:11434"` -> load `llama3` (assuming llama3 is served by local Ollama) -> chat.
    *   Configure `LLM_BACKEND_TYPE="vllm"`, `VLLM_BASE_URL="http://localhost:8000"` -> load the model served by vLLM -> chat.

11. **Review and Refine:** Look for code duplication, improve error handling, make parameter mapping robust. Ensure thread safety (passing config explicitly to `generate` helps). Ensure async operations (API calls) use `await`. Ensure local loading still uses `run_in_executor`.
