## Enhance the `POST /api/v1/sessions/{session_id}/messages` endpoint. ##

**The goal is to add:**  

1.  **RAG Step:**
    *   Retrieve the `rag_document_ids` associated with the current session from the `sessions` table.
    *   If document IDs exist, perform a semantic search using the new user message as the query against the ChromaDB `documents` collection, filtering by those specific document IDs.
    *   Retrieve the top N relevant document chunks.
2.  **Context Formatting:**
    *   Format the retrieved RAG context (the text from the relevant chunks).
3.  **Prompt Construction:**
    *   Retrieve recent chat history from the `chat_messages` table for the current session.
    *   Construct a prompt for the LLM that includes:
        *   System instructions (optional).
        *   Formatted RAG context.
        *   Formatted recent chat history.
        *   The new user message.
4.  **Placeholder LLM Call:**
    *   Instead of calling a real LLM, simulate a response. This could be a simple canned message like "LLM response placeholder based on your query and context." or even echo back the constructed prompt or retrieved context.
5.  **Store Assistant Message:**
    *   Store the placeholder LLM response as a new message with `role='assistant'` in the `chat_messages` table, linked to the same session.
6.  **Return Response:**
    *   Decide what to return to the API client. Options:
        *   Just the assistant's response (`ChatMessageResponse`).
        *   Both the user's saved message and the assistant's response (`List[ChatMessageResponse]`).
        *   A custom response object containing the assistant message and maybe context info. Let's start by returning just the assistant's `ChatMessageResponse`.

**Refinement:**
*   How much chat history to include? Make it configurable or use a simple limit (e.g., last 5 turns).
*   How to format the prompt? Use a clear structure (e.g., markers for context, history, user query).
*   Where should the RAG logic live? It could be directly in the endpoint function for now, or moved to a dedicated `ChatService` later. Let's put it in the endpoint first for clarity.
*   Need access to the embedding function (`generate_embeddings`) and ChromaDB client within the endpoint.
*   Need access to the database (`database`) to fetch session info and chat history.

**Plan:**
1.  Modify the `add_message_to_session` function in `app/api/endpoints/sessions.py`.
2.  Import necessary functions/clients: `get_chroma_client`, `generate_embeddings`, `settings`, `sqlalchemy`, etc.
3.  Fetch session data (including `rag_document_ids`).
4.  Store the incoming user message first (within a transaction).
5.  Perform RAG Search:
    *   Check if `rag_document_ids` exist for the session.
    *   If yes, generate embedding for the user's message content.
    *   Query ChromaDB `documents` collection, filtering by session's `document_ids`.
    *   Handle results.
6.  Retrieve Chat History:
    *   Query `chat_messages` table for the session, limit results, order by timestamp.
7.  Format Context and History: Create strings representing the retrieved information.
8.  Construct the Prompt String (placeholder).
9.  Generate Placeholder LLM Response: Create a simple string.
10. Store Assistant Message: Insert the placeholder response into `chat_messages` table.
11. Update Session Timestamp: This should happen when the *assistant* message is added (or keep the user message add logic). Let's make sure it reflects the *end* of the interaction. The current transaction handles the user message insert + session update. We need another step for the assistant message. Maybe combine into one larger transaction? Or update timestamp again after assistant message? Let's update it again for simplicity.
12. Return the newly created assistant message object.
