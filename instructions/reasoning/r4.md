## Integrate vector embedding into the document processing pipeline ##

**Plan:**

1.  **Choose an Embedding Model:** Select a suitable sentence transformer model from Hugging Face (e.g., `all-MiniLM-L6-v2` for a balance of speed and performance, or a larger one if higher accuracy is needed and resources allow).
2.  **Add Dependencies:** Install `sentence-transformers` and potentially `torch` (if not already installed, depending on the model and hardware).
3.  **Update Configuration:** Add settings for the chosen embedding model name and potentially the device to run it on (CPU/GPU).
4.  **Create Embedding Service:**
    *   Create a new service/module (e.g., `services/embedding_service.py`).
    *   Implement a class or function to load the sentence transformer model. This should ideally happen once on server startup or lazily on first use to avoid reloading repeatedly.
    *   Implement a function to generate embeddings for a list of text chunks. Handle batching for efficiency.
5.  **Integrate with ChromaDB:**
    *   Modify `db/database.py` (or the embedding service) to get the configured ChromaDB collection for documents.
    *   Ensure the collection is retrieved correctly. ChromaDB can optionally be configured with a specific embedding function, which might simplify things, or we can generate embeddings manually and add them. Let's start with manual generation for clarity.
6.  **Modify Document Processing:**
    *   Update the `process_document_upload` background task in `services/document_processor.py`.
    *   After successfully saving chunks to SQLite, retrieve those chunks.
    *   Call the embedding service to generate embeddings for the chunk texts.
    *   Prepare the data for ChromaDB: IDs (use the chunk IDs), embeddings, documents (the chunk text), and metadatas (e.g., `document_id`, `chunk_index`, `filename`).
    *   Use the `chroma_client` to add/upsert the embeddings into the document collection.
    *   Update the `documents` table status in SQLite to `embedding` during the process and `completed` upon success or `embedding_failed` on error.
    *   Update the `document_chunks` table with the corresponding vector ID (using the chunk ID as the vector ID is simplest).
7.  **Error Handling:** Add specific error handling for the embedding process (model loading errors, embedding generation errors, ChromaDB errors).
8.  **Testing:** Upload a document and verify:
    *   Logs show embedding happening.
    *   Document status in SQLite becomes `completed`.
    *   `vector_id` is populated in `document_chunks` table.
    *   Use ChromaDB's API (perhaps via a simple test script or a future API endpoint) to confirm vectors exist in the collection.
