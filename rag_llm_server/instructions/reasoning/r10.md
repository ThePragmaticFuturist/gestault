## Multi-LLM backend integration ##

1.    **Understand the Goal:** The user wants to connect the existing RAG server to *external* LLM inference servers (Ollama, vLLM, potentially InstructLab's server component) in addition to the current Hugging Face local loading mechanism. The server should be able to *switch* between these backends.

2.    **Identify Core Requirements:**
    *   **Configuration:** Need a way to specify *which* backend to use (e.g., "local", "ollama", "vllm") and the necessary connection details (e.g., base URL for API backends).
    *   **Abstraction:** Need a unified interface within the server code so that the chat endpoint (`add_message_to_session`) doesn't need complex `if/elif` statements for each backend. It should just call a generic "generate" function.
    *   **Backend Implementations:** Need specific logic for interacting with each type of backend (local transformers, Ollama API, vLLM OpenAI-compatible API).
    *   **Switching Mechanism:** Need a way to select and activate the desired backend based on the configuration.
    *   **API Interaction:** For API-based backends (Ollama, vLLM), need an HTTP client library (like `httpx`).
    *   **Error Handling:** Consistent error handling across different backends.
    *   **Status Reporting:** The `/models/status` endpoint should reflect which backend is active and its specific details (model name, URL, etc.).

3.  **Design the Abstraction:**
    *   Define a base class or a common function signature for generation. Let's go with a class-based approach for better state management per backend.
    *   Create an abstract base class `LLMBackendBase` with an `async generate(prompt: str, config: Dict[str, Any]) -> Optional[str]` method. Make it `async` because API calls will be async. Pass the generation `config` explicitly.
    *   Subclass this for each backend:
        *   `LocalTransformersBackend(LLMBackendBase)`: Encapsulates the existing `transformers.pipeline` logic. Needs access to the pipeline object.
        *   `OllamaBackend(LLMBackendBase)`: Takes an Ollama base URL and model name. Uses `httpx` to call Ollama's `/api/generate` endpoint. Needs to map parameters.
        *   `VLLMBackend(LLMBackendBase)`: Takes a vLLM base URL and model name. Uses `httpx` to call vLLM's OpenAI-compatible `/v1/completions` or `/v1/chat/completions` endpoint. Needs to map parameters.
        *   `InstructLabBackend(LLMBackendBase)`: (Placeholder for now, assume similar OpenAI-compatible API).

4.  **Refactor `llm_service.py`:**
    *   This service should now manage the *active* backend instance instead of just the local pipeline.
    *   Introduce a configuration setting (`LLM_BACKEND_TYPE`) to choose the backend.
    *   Modify `llm_state` to store the *active backend instance* (`llm_state["backend_instance"]`).
    *   Modify the loading logic:
        *   The `/load` endpoint now needs to know *which type* of backend to load/configure. The `model_name_or_path` might now represent a model *on that backend* (e.g., "llama3" for Ollama) or a path/ID for local.
        *   The `_load_model_task` needs to be replaced or heavily modified. Maybe have separate load functions per backend type? Or a factory function?
        *   Let's simplify: Instead of `/load` dynamically loading different *types*, let's configure the desired backend *type* and its *URL* in `settings`. The `/load` endpoint will then primarily load/configure the *model* for the *currently configured* backend type.
        *   Local loading remains similar (using `run_in_executor`).
        *   API backends don't need "loading" in the same way; they just need configuration (URL, model name). The `/load` endpoint could be used to *select* the model *on* the API backend and maybe verify the connection.
    *   Modify `generate_text`: This function becomes much simpler. It just gets the active `backend_instance` from `llm_state` and calls `await backend_instance.generate(prompt, config)`.
    *   Modify `get_llm_status`: Needs to report the active backend type, URL (if applicable), and the model selected on that backend.
    *   Modify `update_llm_config`: Should update the config stored in `llm_state`, which will be passed to the active backend's `generate` method.

5.  **Update Configuration (`config.py`):**
    *   Add `LLM_BACKEND_TYPE: str = Field(default="local", description="'local', 'ollama', 'vllm'")`
    *   Add `OLLAMA_BASE_URL: Optional[str] = Field(default=None, description="Base URL for Ollama API (e.g., http://localhost:11434)")`
    *   Add `VLLM_BASE_URL: Optional[str] = Field(default=None, description="Base URL for vLLM OpenAI-compatible API (e.g., http://localhost:8000)")`
    *   The existing `DEFAULT_MODEL_NAME` might need context â€“ is it the local path or the model name on the API server? Let the `/load` request specify the model relevant to the active backend.

6.  **Implement API Backends:**
    *   Install `httpx`: `pip install httpx`.
    *   **Ollama:**
        *   Need the `/api/generate` endpoint structure (model, prompt, stream=False, options={...}).
        *   Map our config (temp, top_k, etc.) to Ollama's `options`.
        *   Handle potential errors from the API call.
    *   **vLLM (OpenAI compatible):**
        *   Decide between `/completions` (legacy) or `/chat/completions`. `/chat/completions` is generally preferred now.
        *   Need to format the prompt into the chat message structure (`messages=[{"role": "user", "content": prompt}]`).
        *   Map our config to OpenAI parameters (temperature, top_p, max_tokens). Note `top_k` isn't standard in OpenAI chat API.
        *   Handle API errors.

7.  **Refine `load_model` Endpoint:**
    *   The request body should still contain `model_name_or_path`.
    *   The endpoint logic checks `settings.LLM_BACKEND_TYPE`.
    *   If "local": Calls the existing local loading task (maybe renamed `_load_local_model_task`).
    *   If "ollama" or "vllm":
        *   Checks if the corresponding base URL is configured.
        *   Updates `llm_state` with the chosen model name, backend type, and URL. Sets status to `LOADED` (since no real loading needed, just configuration/selection).
        *   Maybe perform a quick health check ping to the API endpoint? (e.g., list models on Ollama/vLLM).

8.  **Update API Models (`llm.py`):**
    *   `ModelLoadRequest`: Keep `model_name_or_path`. Remove `device` and `quantization` as they are less relevant/handled differently for API backends (or make them optional).
    *   `ModelStatusResponse`: Add `backend_type`, `backend_url`. Rename `model_name_or_path` to `active_model`.

9.  **Refactor `sessions.py`:** No changes should be needed here, as it calls the abstracted `generate_text` from `llm_service`.

10. **Testing:** Test each backend type:
    *   Configure `LLM_BACKEND_TYPE="local"` -> load `gpt2` -> chat.
    *   Configure `LLM_BACKEND_TYPE="ollama"`, `OLLAMA_BASE_URL="http://localhost:11434"` -> load `llama3` (assuming llama3 is served by local Ollama) -> chat.
    *   Configure `LLM_BACKEND_TYPE="vllm"`, `VLLM_BASE_URL="http://localhost:8000"` -> load the model served by vLLM -> chat.

11. **Review and Refine:** Look for code duplication, improve error handling, make parameter mapping robust. Ensure thread safety (passing config explicitly to `generate` helps). Ensure async operations (API calls) use `await`. Ensure local loading still uses `run_in_executor`.

## modify the server to select the backend type via the API request ##  

1.  **Understand the Goal:** The user wants to select the LLM backend (`local`, `ollama`, `vllm`) dynamically via the `POST /models/load` API call, rather than relying solely on server configuration (`.env` or environment variables). The server should default to the `local` backend if no type is specified in the request.

2.  **Identify Affected Components:**
    *   **Configuration (`core/config.py`):** The `LLM_BACKEND_TYPE` setting is no longer the *sole determinant*. It can serve as a default, but the API request needs to override it. We might keep it as a default fallback or remove it if the API *must* specify the type. Let's keep it as a default for now. The Base URLs (Ollama, vLLM) are still needed in the config. `DEFAULT_MODEL_NAME_OR_PATH` also remains relevant as a fallback if the API request doesn't specify a model.
    *   **API Models (`app/api/models/llm.py`):** The `ModelLoadRequest` needs a new field to specify the desired `backend_type`. This field should be optional, defaulting to `local`.
    *   **API Endpoint (`app/api/endpoints/models.py`):** The `load_or_set_model` endpoint needs to:
        *   Accept the new `backend_type` from the request body.
        *   Determine the final `backend_type` to use (request value, or server default `settings.LLM_BACKEND_TYPE`, or hardcoded 'local' default).
        *   Pass this determined `backend_type` to the `set_active_backend` service function.
    *   **Service Layer (`services/llm_service.py`):**
        *   The `set_active_backend` function already accepts `backend_type` as an argument, so its signature is okay.
        *   The global `llm_state` will store the `backend_type` that was *actually* activated.
        *   The `get_llm_status` function already returns the active `backend_type`.
    *   **Backend Implementations (`services/llm_backends.py`):** No changes needed here; they are instantiated based on the type passed to `set_active_backend`.

3.  **Step-by-Step Implementation Plan:**

    *   **Modify `core/config.py`:** Adjust the description of `LLM_BACKEND_TYPE` to indicate it's now a default/fallback.
    *   **Modify `app/api/models/llm.py`:** Add `backend_type: Optional[Literal["local", "ollama", "vllm", "instructlab"]] = None` to `ModelLoadRequest`.
    *   **Modify `app/api/endpoints/models.py` (`load_or_set_model`):**
        *   Read `load_request.backend_type`.
        *   Establish the precedence: Use `load_request.backend_type` if provided, otherwise fall back to `settings.LLM_BACKEND_TYPE`, potentially with a final fallback to `"local"`.
        *   Pass the chosen `backend_type` to `await set_active_backend(...)`.
    *   **Testing:** Test loading with and without the `backend_type` in the request body to verify default and override behavior.

4.  **Refinement - Defaulting:** How should the default work precisely?
    *   Should the API *require* the type? No, the request is to make it optional and default to local.
    *   Should the server config `LLM_BACKEND_TYPE` be the primary default? Yes, that seems reasonable. If the user configured the server for Ollama by default, but doesn't specify in the API, it should still use Ollama.
    *   What if the server config *isn't* set? Should it default to `local`? Yes, that's a safe fallback.

    *Revised Default Logic in `load_or_set_model`*:*
    ```python
    requested_type = load_request.backend_type
    config_default_type = settings.LLM_BACKEND_TYPE
    final_backend_type = requested_type or config_default_type # Use request if provided, else config default
    # Handle case where config default might also be invalid/None if not set strictly
    if not final_backend_type:
         final_backend_type = "local" # Ultimate fallback
    ```
    *Self-correction:* `Literal` in Pydantic/config will likely enforce that `settings.LLM_BACKEND_TYPE` *is* one of the valid types if set. So the fallback logic can be simpler: `final_backend_type = load_request.backend_type or settings.LLM_BACKEND_TYPE`.

5.  **Refinement - Model Name:** The `model_name_or_path` also needs a default. If the request doesn't provide it, we should use `settings.DEFAULT_MODEL_NAME_OR_PATH`. This needs to be handled in the endpoint as well.

6.  **Final Code Structure Review:** Look at the planned changes again. Do they interact correctly? Yes, the endpoint becomes the decision point for *which* backend type and *which* model ID to pass to the service layer, leveraging configuration for defaults. The service layer remains mostly unchanged in its core logic, just receiving the decided parameters.

7.  **Write Code:** Implement the changes based on the refined plan. Add logging to show which backend type is being used (request override vs. config default).

8.  **Document Testing:** Outline the test cases needed (no type in request, explicit type in request, no model name in request, etc.).
