## Add code to initiate hugginface cli login ##

Here's a thinking process for adding the Hugging Face login check:

1.  **Understand the Problem:** The user encountered an `OSError` combined with `huggingface_hub.errors.RepositoryNotFoundError` when trying to load `llama3`. The error message clearly states that the model identifier isn't found on the Hub *or* suggests it might be a private/gated model requiring authentication. The user explicitly asked to ensure the Hugging Face CLI login is completed *before* the server starts doing anything else (like loading models).

2.  **Identify the Goal:** We need a mechanism to check if the user is logged into Hugging Face Hub (meaning a token is available locally) *at server startup*. If not logged in, the server should ideally either:
    *   Log a strong warning and potentially continue (but model loading for private/gated models will fail later).
    *   Refuse to start, forcing the user to log in first. (This seems safer and more aligned with the user's request "ensure ... login is completed").

3.  **Find the Right Tool:** How can we programmatically check the Hugging Face login status? The `huggingface_hub` library itself should provide this functionality.
    *   Recall or search `huggingface_hub` documentation/API reference for authentication or token management. Keywords: "huggingface_hub check login", "huggingface_hub get token", "huggingface_hub whoami".
    *   The `huggingface_hub.whoami()` function seems promising. It tries to retrieve user information using the stored token. If it succeeds, the user is logged in. If it fails (e.g., raises an exception or returns specific error info), they are not.
    *   Alternatively, `huggingface_hub.get_token()` could be used. If it returns a token string, they are logged in. If it returns `None`, they are not. This seems simpler and less prone to network errors than `whoami()`.

4.  **Choose the Check Location:** Where in the server startup process should this check happen?
    *   It needs to run *early*, before any attempt to load models that might require authentication.
    *   The FastAPI `startup` event handler in `app/main.py` is a good place. It runs after the initial application setup but before the server starts accepting requests, and crucially *before* we trigger any model loading (like the embedding model pre-load or any potential default LLM pre-load).
    *   Putting it right at the beginning of the `startup_event` function seems appropriate.

5.  **Implement the Check:**
    *   Import `get_token` from `huggingface_hub`.
    *   Import `sys` to allow exiting the application if login fails.
    *   Inside `startup_event`:
        *   Call `get_token()`.
        *   Check if the result is `None`.
        *   If `None`:
            *   Print a clear error message instructing the user to run `huggingface-cli login`.
            *   Use `sys.exit(1)` to terminate the server startup process with a non-zero exit code, indicating failure.
        *   If not `None`:
            *   Print a confirmation message that the user is logged in.
            *   Continue with the rest of the startup process (DB connection, embedding model load, etc.).

6.  **Refine the Code (Error Handling, Imports):**
    *   Ensure necessary imports (`huggingface_hub`, `sys`, `logging` or `print`) are in `app/main.py`.
    *   Wrap the check logic clearly.

7.  **Consider Alternatives (and why the chosen method is good):**
    *   *Trying to log in programmatically:* Generally discouraged for security reasons (handling user credentials). Using the CLI's stored token is standard practice.
    *   *Passing token via config:* Possible (`token=<your_token>` argument in `from_pretrained`), but less secure and less convenient than `huggingface-cli login`. The check ensures the standard login method has been used.
    *   *Just warning instead of exiting:* Less robust. The user requested *ensuring* login. Exiting prevents unexpected failures later when trying to load a gated model.

8.  **Final Code Structure (in `app/main.py` `startup_event`):**
    ```python
    from huggingface_hub import get_token
    import sys
    # ... other imports ...

    @app.on_event("startup")
    async def startup_event():
        print("--- Running Server Startup ---") # Add a clear marker

        # --- Check Hugging Face Login ---
        print("--> Checking Hugging Face token...")
        hf_token = get_token()
        if hf_token is None:
            print("\n" + "="*50)
            print("ERROR: Hugging Face token not found.")
            print("Please log in using the Hugging Face CLI:")
            print("  huggingface-cli login")
            print("Exiting server startup.")
            print("="*50 + "\n")
            sys.exit(1) # Exit with error code
        else:
            print("--> Hugging Face token found. Login verified.")
        # --- End Check ---

        # --- Original Startup Steps ---
        print("--> Connecting to database...")
        await connect_db()
        # ... rest of startup ...
    ```

9.  **Documentation/Explanation:** Explain *why* this check is necessary (private/gated models), how it works (`get_token`), where it's placed, and what the user needs to do (`huggingface-cli login`). Also, explicitly mention the `llama3` model requires authentication as it's gated.
