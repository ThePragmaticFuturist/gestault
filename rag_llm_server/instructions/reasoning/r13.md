# Here's a thinking process for designing the long-term memory feature using RAG on chat history #

1.  **Understand the Goal:** The user wants to persist chat history not just for viewing, but as a searchable knowledge base (long-term memory) *within* a specific chat session. When a user sends a new prompt, the system should search the *past* history of *that session*, retrieve relevant prior interactions, potentially summarize them, and add them to the context sent to the LLM, alongside any RAG document context. This needs to work even after loading a saved session.

2.  **Identify Key Challenges & Requirements:**
    *   **Storage:** Where and how to store the chat history for efficient search? Using ChromaDB (our existing vector store) makes sense. We need a way to distinguish chat history vectors from document vectors and associate them with a specific session.
    *   **Indexing:** When should chat turns be indexed (embedded and stored)? After each user/assistant pair? Immediately? In the background?
    *   **Retrieval:** How to trigger the search? On every user prompt? How to formulate the search query (use user prompt? use summarized history?)?
    *   **Context Integration:** How to combine retrieved chat history context with retrieved document context and current chat history within the prompt?
    *   **Token Limits:** Both retrieved history and document context add to the prompt length. Summarization of retrieved history becomes crucial.
    *   **Session Association:** All indexed history must be strictly tied to the correct session ID. Retrieval must only fetch history from the *current* session.
    *   **Concurrency:** If indexing happens immediately, ensure it doesn't block the response to the user. Background tasks are needed.

3.  **Design Choices & Strategy:**

    *   **Storage:** Use ChromaDB. Create a *separate collection* for chat history (e.g., `chat_sessions_history`) or use the *same collection* as documents but with distinct metadata (e.g., adding `session_id` and `message_role` to metadata). Using the *same* collection might simplify retrieval if we want to search across both docs and history simultaneously later, but requires careful filtering. Let's start with the *same* collection (`rag_documents`) but add specific metadata (`type: 'chat'`, `session_id`, `message_id`, `role`). This allows unified search later.
    *   **Indexing Trigger:** Indexing *after* the assistant responds seems logical. We have the complete user-assistant turn.
    *   **Indexing Unit:** Should we embed individual messages or user-assistant pairs? Embedding individual messages (both user and assistant) provides more granularity for retrieval. We'll need unique IDs for each message vector.
    *   **Indexing Process:** Create a background task triggered after the assistant message is stored in SQLite. This task will:
        *   Fetch the user and assistant messages just added.
        *   Generate embeddings for their content.
        *   Prepare metadata (`type: 'chat'`, `session_id`, `message_id`, `role`).
        *   Add the embeddings, text, and metadata to the ChromaDB collection.
    *   **Retrieval Trigger:** On each new user prompt (`handleSendMessage` / `add_message_to_session`).
    *   **Retrieval Query:** Use the current user prompt text to generate a query embedding.
    *   **Retrieval Filtering:** Query ChromaDB using the query embedding BUT add a `where` filter: `{"$and": [{"type": "chat"}, {"session_id": current_session_id}]}` to fetch only chat history from the *current* session. Retrieve top-k relevant past messages.
    *   **Summarization:** Similar to document context summarization, if retrieved chat history snippets are too long or numerous, use the LLM (or a dedicated one) to summarize them *before* adding to the prompt context. Prompt the summarizer with the *current* user query and the retrieved history snippet.
    *   **Prompt Construction:** Modify the prompt building logic to include sections for:
        *   `### INSTRUCTION:`
        *   `### RETRIEVED DOCUMENT CONTEXT (Summarized):` (if applicable)
        *   `### RELEVANT PAST CONVERSATION (Summarized):` (if applicable)
        *   `### RECENT CHAT HISTORY:` (short-term window, maybe 2-3 pairs)
        *   `### USER QUERY:`
        *   `### RESPONSE:`
    *   **Token Management:** The existing truncation logic needs to account for the *combined* length of document summaries, history summaries, and recent history. Prioritize which context to truncate first (e.g., older history summaries before document summaries?).
    *   **Session Loading:** When loading a session from `localStorage`, no immediate action is needed regarding the indexed history, as it's already stored in ChromaDB associated with the backend session ID. The retrieval process will automatically pick it up on the next user prompt.

4.  **Refine Implementation Plan (Backend Changes):**

    *   **`db/models.py`:** No changes needed immediately, but consider if the `chat_messages` table needs an `is_embedded` status flag for robustness.
    *   **`services/document_processor.py` (Rename/Refactor?):** Maybe rename this or create a new `services/indexing_service.py`. Let's add a new function `index_chat_turn` to the existing `document_processor.py` for now, but acknowledge it might deserve its own module. This function will be the background task.
    *   **`services/embedding_service.py`:** No changes needed.
    *   **`services/llm_service.py` & `llm_backends.py`:** Need the summarization capability (we already added `summarize_text_with_query`, but need to consider the thread-safety issue if using local backend heavily).
    *   **`app/api/endpoints/sessions.py` (`add_message_to_session`):**
        *   After storing the assistant message, trigger the `index_chat_turn` background task, passing relevant message IDs or content.
        *   *Before* constructing the prompt for the *current* user query:
            *   Perform the semantic search for *chat history* on ChromaDB (using the user query embedding and session ID filter).
            *   Retrieve top-k past messages.
            *   If history found, loop through snippets and call `summarize_text_with_query` (running in executor).
            *   Combine summaries into `retrieved_history_context`.
        *   Modify prompt construction to include the new `retrieved_history_context` section.
        *   Modify truncation logic to consider the total length including document context summaries *and* history context summaries.

5.  **Refine Implementation Plan (Frontend Changes):**

    *   Mostly UI/display changes, if desired.
    *   Potentially add visual cues indicating that long-term memory is being searched or used.
    *   No fundamental changes needed to session saving/loading logic, as the memory resides on the backend.

6.  **Address Concurrency:** Reiterate the warning about the current summarization approach modifying global config. Suggest alternatives (passing config overrides, separate pipeline) for production.

7.  **Structure the Answer:** Start with the concept, explain the backend changes (storage, indexing task, retrieval/summarization in endpoint), then mention frontend (minimal changes needed), and finally discuss trade-offs (latency, concurrency).
